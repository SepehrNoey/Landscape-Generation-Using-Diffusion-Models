# Landscape Image Generation Using Diffusion Model

This project implements a diffusion model based on the architecture proposed in the [DDPM paper](https://arxiv.org/abs/2006.11239). The model is designed to generate 32x32 landscape images using the UNet architecture, with self-attention, upsampling, and downsampling blocks implemented.
## Sample Generated Images

### Final Results
Here are some landscape images generated by the model after training. Notice that each single generated image is a `32*32` image, and they are placed together in the following images.

|Sampling 1|Sampling 2|
|----|----|
|![3](https://github.com/user-attachments/assets/7dbf4d05-eb98-443c-9398-16f7a03e98db)|![4](https://github.com/user-attachments/assets/52f07940-6764-40f2-963e-12ae5f61659a)|

### Generated Images in Different Epochs
The flow of learning to generate realistic images of landscapes can be easily seen in the following table:
|Epoch #|Generated Images|
|----|----|
|Epoch 1|![0 (2)](https://github.com/user-attachments/assets/a1d733fe-d0f0-4025-83bd-8defe252a67a)|
|Epoch 50|![0 (3)](https://github.com/user-attachments/assets/238da735-a28a-4769-86b1-4056b3be7130)|
|Epoch 100|![0 (5)](https://github.com/user-attachments/assets/2464dc68-6435-40c9-8224-e8395a1bf9bb)|
|Epoch 150|![0 (4)](https://github.com/user-attachments/assets/79677bb2-6fc9-4344-8730-19159e3d3ce2)|
|Epoch 300|![0 (6)](https://github.com/user-attachments/assets/366c6ae1-5319-41b0-85d3-79c524e4de83)|
|Epoch 500|![0 (7)](https://github.com/user-attachments/assets/2223d4a1-b607-4591-a83c-44022a0ed983)|

## Implementation Details
1- The `Diffusion` class: This class is a wrapper around the `Unet` class and implements the forward and reverse process in diffusion models (adding noise and denoising) which at last generates new images.

- **Noise Steps**: 1000 steps of noise addition
- **Noise Schedule**: Linear schedule for beta values between `1e-4` and `0.02`
- **Image Size**: `32x32` input and output
- **Sampling**: Images are sampled from random noise from normal distribution and denoised through reverse diffusion

2- `UNet` Architecture: The `UNet` class is the concrete model that learns to predict the noise in images. It includes several blocks like downsampling, upsampling, and self-attention blocks to capture both local and global features in the images.

- **DoubleConv**: Two convolutional layers with GroupNorm and GELU activation, used throughout the network.
- **Down**: Downsampling block that reduces the spatial resolution while increasing the feature maps.
- **Up**: Upsampling block to restore the spatial resolution.
- **SelfAttention**: Responsible for implementing attention mechanism in the model.

## Training Details

The diffusion model was trained on a landscape image dataset using the following configurations:

- **Learning Rate**: 3e-4
- **Optimizer**: AdamW
- **Loss Function**: Mean Squared Error (MSE)
- **Number of Epochs**: 500
- **Batch Size**: 24
- **Input Shape**: 32 x 32 (RGB)
- **Output Shape**: 32 x 32 (RGB)
- **Beta Noise Schedule**: Linear schedule between `1e-4` and `0.02` for the forward process of adding noise during diffusion.
- **Model Architecture**: UNet + Self-Attentions layers

### Hardware and Time

- **Hardware**: The model was trained on a Kaggle notebook with a P100 GPU.
- **Training Time**: Around 13 hours

### Dataset

- **Dataset Used**: The dataset consisted around 4200 landscape images, resized to 32x32 pixels due to limited resource and time for training. The dataset can be found [here](https://www.kaggle.com/datasets/arnaud58/landscape-pictures).

### Trainin loss
The training loss over around `90000` iterations can be seen in the following plot:

![training_loss](https://github.com/user-attachments/assets/7d101aee-71b8-4326-8b9c-4847ebc1e339)
